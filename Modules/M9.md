# 9 - MCTS & Reward Shaping & Q-Function Approximation



## 知识点 & [题目](#题目)

### MCTS

##### Value iteration and TD learning work will in some situations, however

* Value iteration learns a policy, but it requires a model; and also learns a policy for every state → lots of states, X
* TD-learning learns a policy, but really only works well for repetitive tasks OR requires extensive training (e.g. up to weeks) → cannot react to things that were not explored a lot!

They are both **offline methods**: policies must be trained in advance.

##### MCTS: Similar to Monte-Carlo reinforcement learning (based on simulation), but uses a tree to prioritise more favourable moves

##### MCTS Foundation: MDPs as Expecti-Max Trees

<img src="images/9/9-1.jpg" alt="9-1" style="zoom:67%;" />

##### MCTS Overview

<img src="images/9/9-2.jpg" alt="9-2" style="zoom:80%;" />

##### Example

![9-19](images/9/9-19.jpg)

##### Upper Confidence Trees: effective as the multi-armed bandit

UCT = MCTS + UCB1

<img src="images/9/9-3.jpg" alt="9-3" style="zoom: 80%;" />

##### MCTS Large Example	L9.1 P9

MCTS is particularly effective in massive state/search spaces when resources and time are not available.



### Reward Shaping

<img src="images/9/9-4.jpg" alt="9-4" style="zoom:80%;" />

<img src="images/9/9-5.jpg" alt="9-5" style="zoom:80%;" />

#### Example

<img src="images/9/9-7.jpg" alt="9-7" style="zoom:80%;" /><img src="images/9/9-6.jpg" alt="9-6" style="zoom:75%;" />

<img src="images/9/9-8.jpg" alt="9-8" style="zoom:80%;" />

#### Q-Function Initialisation

<img src="images/9/9-9.jpg" alt="9-9" style="zoom:80%;" />

* Small, ‘fake’ rewards can frequently be easily defined using domain knowledge.
* If defined as a potential functions, guaranteed to converge.
* A well defined potential function can significantly reduce the length of episodes early in learning.
* Q-function initialisation - similar technique, shown to have equivalent outcomes if potential function is static.



### Q-Function Approximation

##### Using Q-function approximation, we can scale reinforcement learning by approximating Q functions, rather than storing complete Q tables.

#### Linear Q-Function Approximation

##### Use simple linear methods in which we select features and learn weights are effective and guarantee convergence.

##### Representation:<img src="images/9/9-10.jpg" alt="9-10" style="zoom:80%;" />

##### Update:<img src="images/9/9-11.jpg" alt="9-11" style="zoom:80%;" />

* w: weight

##### Example: Freeway	L9.3 P5

##### Defining state-action features

<img src="images/9/9-12.jpg" alt="9-12" style="zoom:80%;" />

#### Deep Q-Learning

##### Offers alternatives in which we do not need to select features, making it more suitable for unstructured data. But requires more training data (more episodes) and has no convergence guarantees.

<img src="images/9/9-13.jpg" alt="9-13" style="zoom:67%;" />

* theta - > gradient

#### Strengths ✔ & Limitations ❌

**Q-function approximation**

* Compact representation ✔
* Q-value propagation ✔
* They are approximations ❌

**Linear**

* Convergence ✔

**Deep**

* Minimal feature engineering ✔
* Unstructured input ✔
* Data hungry (high sampling complexity) ❌



## 题目

### Quiz

<img src="images/9/9-14.jpg" alt="9-14" style="zoom:80%;" />

![9-15](images/9/9-15.jpg)

<img src="images/9/9-16.jpg" alt="9-16" style="zoom:80%;" />

<img src="images/9/9-17.jpg" alt="9-17" style="zoom:80%;" />

<img src="images/9/9-18.jpg" alt="9-18" style="zoom:80%;" />