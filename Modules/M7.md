# 7 - MDPs and Value Iteration



## 知识点 & [题目](#题目)

<img src="images/7/7-1.jpg" alt="7-1"  />

#### Policies: Deterministic vs. Stochastic

Deterministic: pi(s) -> A. Given state s, the policy pi is a function that maps states to actions.

* It specifies which action to choose in every possible state.
* Thus, if we are in state s, our agent should choose the action defined by π(s).

Stochastic: pi(s, a) S * A -> R. Given a state s and action a, returns the probability that action a will be selected in s. Intuitively, π(s, a) specifies the probability that action a should be executed in state s.

#### *<u>Optimal solutions to MDPs: The Bellman Equation (Discounted-Reward MDPs)</u>*

<img src="images/7/7-2.jpg" alt="7-2" style="zoom: 80%;" />

#### <u>*Solving MDPs with Dynamic Programming: Value Iteration*</u>

![7-3](images/7/7-3.jpg)

#### <img src="images/7/7-4.jpg" alt="7-4"  />

##### O(|S|^2 |A| n)	L7 P16

#### Policy extraction:![7-5](images/7/7-5.jpg)

#### Summary

* We covered Markov Decision Processes (MDPs). They differ from classical planning in that **actions can have more than one possible outcome**. Each outcome has an associated probability.
* The optimal policy can be computed through value iteration, which is based on dynamic programming. Specifically, it uses the Bellman equations to iteratively improve on a non-optimal solution.
* We looked at how to extract policies from value functions derived by value iteration.



## 题目

### Quiz

<img src="images/7/7-6.jpg" alt="7-6" style="zoom: 69%;" /><img src="images/7/7-7.jpg" alt="7-7" style="zoom: 59%;" />

<img src="images/7/7-8.jpg" alt="7-8" style="zoom:65%;" /><img src="images/7/7-9.jpg" alt="7-9" style="zoom:80%;" />

<img src="images/7/7-10.jpg" alt="7-10" style="zoom: 60%;" /><img src="images/7/7-11.jpg" alt="7-11" style="zoom:77%;" />

